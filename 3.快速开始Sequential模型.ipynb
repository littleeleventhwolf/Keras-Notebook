{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 快速开始Sequential模型\n",
    "\n",
    "Sequential是多个网络层的线性堆叠。\n",
    "\n",
    "可以通过向Sequential模型传递一个layer的list来构造该模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "        Dense(32, input_dim=784),\n",
    "        Activation('relu'),\n",
    "        Dense(10),\n",
    "        Activation('softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以通过.add()方法一个个的将layer加入模型中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 指定输入数据的shape\n",
    "\n",
    "模型需要知道输入数据的shape，因此，Sequential的第一层需要接受一个关于输入数据shape的参数，后面的各个层则可以自动地推导出中间数据的shape，因此不需要为每个层都指定这个参数。有几种方法来为第一层指定输入数据的shape。\n",
    "\n",
    "* 传递一个input_shape的关键字参数给第一层，input_shape是一个tuple类型的数据，其中也可以填入None，如果填入None则表示此位置可能是任何正整数。数据的batch大小不应包含在其中。\n",
    "* 传递一个batch_input_shape的关键字参数给第一层，该参数包含数据的batch大小。该参数在指定固定大小batch时比较有用，例如在stateful RNNs中。事实上，Keras内部会通过添加一个None将input_shape转化为batch_input_shape。\n",
    "* 有些2D层，如Dense，支持通过指定其输入维度input_dim来隐含的指定输入数据shape。一些3D的时域层支持通过参数input_dim和input_length来指定输入shape。\n",
    "\n",
    "下面的三个指定输入数据shape的方法是严格等价的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(784, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, batch_input_shape=(None, 784)))\n",
    "# note that batch dimension is \"None\" here,\n",
    "# so the model will be able to process batches of any size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面三种方法也是严格等价的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(10, 64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, batch_input_size=(None, 10, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, input_length=10, input_dim=64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge层\n",
    "\n",
    "多个Sequential可经由一个Merge层合并到一个输出。Merge层的输出是一个可以被添加到新Sequential的层对象。下面这个例子将两个Sequential合并到一起："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Merge\n",
    "\n",
    "left_branch = Sequential()\n",
    "left_branch.add(Dense(32, input_dim=784))\n",
    "\n",
    "right_branch = Sequential()\n",
    "right_branch.add(Dense(32, input_dim=784))\n",
    "\n",
    "merged = Merge([left_branch, right_branch], mode='concat')\n",
    "\n",
    "final_mode = Sequential()\n",
    "final_mode.add(merged)\n",
    "final_mode.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](3-attach-files/two_branches_sequential_model.png)\n",
    "\n",
    "Merge层支持一些预定义的合并模式，包括：\n",
    "\n",
    "* sum(default)：逐元素相加\n",
    "* concat：张量串联，可以通过concat_axis的关键字参数指定按照哪个轴进行串联\n",
    "* mul：逐元素相乘\n",
    "* ave：张量平均\n",
    "* dot：张量相乘，可以通过dot_axis关键字参数来指定要消去的轴\n",
    "* cos：计算2D张量(即矩阵)中各个向量的余弦距离\n",
    "\n",
    "这个两个分支的模型可以通过下面的代码训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "final_model.fit([input_data_1, input_data_2], targets) # we pass one data array per model input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以为Merge层提供关键字参数mode，以实现任意的变换，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = Merge([left_branch, right_branch], mode=lambda x: x[0] - x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在你已经学会定义几乎任何Keras的模型了，对于不能通过Sequential和Merge组合生成的复杂模型，可以参考下节介绍的泛型模型。\n",
    "\n",
    "# 编译\n",
    "\n",
    "在训练模型之前，我们需要通过compile来对学习过程进行配置。compile接收三个参数：\n",
    "\n",
    "* 优化器optimizer：该参数可指定为已预订义的优化器名，如rmsprop、adagrad、或一个Optimizer类的对象。\n",
    "* 损失函数loss：该参数为模型试图最小化的目标函数，它可为预定义的损失函数名，如categorical_crossentropy、mse，也可以为一个损失函数。\n",
    "* 指标列表metrics：对分类问题，我们一般将该列表设置为metrics=['accuracy']。指标可以是一个预定义指标的名字，也可以是一个用户定制的函数。指标函数应该返回单个张量，或一个完成metric_name -> metric_value映射的字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for a multi-class classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# for a binary classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# for a mean squared error regression problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='mse')\n",
    "\n",
    "# for custom metrics\n",
    "import keras.backend as K\n",
    "\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n",
    "def false_rates(y_true, y_pred):\n",
    "    false_neg = ...\n",
    "    false_pos = ...\n",
    "    return {\n",
    "        'false_neg': false_neg,\n",
    "        'false_pos': false_pos,\n",
    "    }\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy', mean_pred, false_rates])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练\n",
    "\n",
    "Keras以Numpy数组作为输入数据和标签的数据类型。训练模型一般使用fit函数。下面是一些例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for a single-input model with 2 classes (binary)\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=784, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 784))\n",
    "labels = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "# train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, labels, nb_epoch=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for a multi-input model with 10 classes\n",
    "\n",
    "left_branch = Sequential()\n",
    "left_branch.add(Dense(32, input_dim=784))\n",
    "\n",
    "right_branch = Sequential()\n",
    "right_branch.add(Dense(32, input_dim=784))\n",
    "\n",
    "merged = Merge([left_branch, right_branch], mode='concat')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(merged)\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# generate dummy data\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "data_1 = np.random.random((1000, 784))\n",
    "data_2 = np.random.random((1000, 784))\n",
    "\n",
    "# these are integers between 0 and 9\n",
    "labels = np.random.randint(10, size=(1000, 1))\n",
    "# we convert the labels to a binary matrix of size (1000, 10)\n",
    "# for use with categorical_crossentropy\n",
    "labels = to_categorical(labels, 10)\n",
    "\n",
    "# train the model\n",
    "# note that we are passing a list of Numpy arrays as training data\n",
    "# since the model has 2 inputs\n",
    "model.fit([data_1, data_2], labels, nb_epoch=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 例子\n",
    "\n",
    "这里是一些帮助你开始的例子。\n",
    "\n",
    "在Keras代码包的examples文件夹中，你将找到使用真实数据的示例模型：\n",
    "\n",
    "* CIFAR10小图片分类：使用CNN和实时数据提升\n",
    "* IMDB电影评论观点分类：使用LSTM处理成序列的词语\n",
    "* Reuters(路透社)新闻主题分类：使用多层感知器(MLP)\n",
    "* MNIST手写数字识别：使用多层感知器和CNN\n",
    "* 字符级文本生成：使用LSTM\n",
    "* ……\n",
    "\n",
    "## 基于多层感知器的softmax多分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 20-dimensional vectors.\n",
    "model.add(Dense(64, input_dim=20, init='uniform'))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, init='uniform'))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, init='uniform'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=sgd,\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fix(X_train, y_train, \n",
    "         nb_epoch=20,\n",
    "         batch_size=16)\n",
    "score = model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相似MLP的另一种实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=20, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adadelta',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用于二分类的多层感知器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=20, init='uniform', activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类似VGG的卷积神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (3, 100, 100) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=(3, 100, 100)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "# Note: Keras does automatic shape inference.\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=32, nb_epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用LSTM的序列分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 256, input_length=maxlen))\n",
    "model.add(LSTM(output_dim=128, activation='sigmoid', inner_activation='hard_sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fix(X_train, Y_train, batch_size=16, nb_epoch=10)\n",
    "score = model.evaluate(X_test, Y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用带门限的递归单元进行图像描述：\n",
    "\n",
    "(单词级别嵌入，描述语句最多16个单词)\n",
    "\n",
    "注意，要使该网络良好工作需要大规模的卷积神经网络并以预训练权重初始化，此处仅为结构示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_caption_len = 16\n",
    "vocab_size = 10000\n",
    "\n",
    "# first, let's define an image model that\n",
    "# will encode pictures into 128-dimensional vectors.\n",
    "# it should be initialized with pre-trained weights.\n",
    "image_model = Sequential()\n",
    "image_model.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=(3, 100, 100)))\n",
    "image_model.add(Activation('relu'))\n",
    "image_model.add(Convolution2D(32, 3, 3))\n",
    "image_model.add(Activation('relu'))\n",
    "image_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "image_model.add(Convolution2D(64, 3, 3, border_mode='valid'))\n",
    "image_model.add(Activation('relu'))\n",
    "image_model.add(Convolution2D(64, 3, 3))\n",
    "image_model.add(Activation('relu'))\n",
    "image_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "image_model.add(Flatten())\n",
    "image_model.add(Dense(128))\n",
    "\n",
    "# let's load the weights from a save file.\n",
    "image_model.load_weights('weights_file.h5')\n",
    "\n",
    "# next, let's define a RNN model that encodes sequences of words\n",
    "# into sequences of 128-dimensional word vectors.\n",
    "language_model = Sequential()\n",
    "language_model.add(Embedding(vocab_size, 256, input_length=max_caption_len))\n",
    "language_model.add(GRU(output_dim=128, return_sequences=True))\n",
    "language_model.add(TimeDistributed(Dense(128)))\n",
    "\n",
    "# let's repeat the image vector to turn it into a sequence.\n",
    "image_model.add(RepeatVector(max_caption_len))\n",
    "\n",
    "# the output of both models will be tensors of shape (samples, max_caption_len, 128).\n",
    "# let's concatenate these 2 vector sequences.\n",
    "model = Sequential()\n",
    "model.add(Merge([image_model, language_model], mode='concat', concat_axis=1))\n",
    "# let's encode this vector sequence into a single vector\n",
    "model.add(GRU(256, return_sequences=False))\n",
    "# which will be used to compute a probability\n",
    "# distribution over what the next word in the caption should be!\n",
    "model.add(Dense(vocab_size))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# \"images\" is a numpy float array of shape (nb_samples, nb_channels=3, width, height).\n",
    "# \"captions\" is a numpy integer array of shape (nb_samples, max_caption_len)\n",
    "# containing word index sequences representing partial captions.\n",
    "# \"next_words\" is a numpy float array of shape (nb_samples, vocab_size)\n",
    "# containing a categorical encoding (0s and 1s) of the next word in the corresponding\n",
    "# partial caption.\n",
    "model.fit([images, partial_captions], next_words, batch_size=16, nb_epoch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用于序列分类的栈式LSTM\n",
    "\n",
    "在该模型中，我们将三个LSTM堆叠在一起，使该模型能够学习更高层次的时域特征表示。\n",
    "\n",
    "开始的两层LSTM返回其全部输出序列，而第三层LSTM只返回其输出序列的最后一步结果，从而其时域维度降低(即将输入序列转换为单个向量)。\n",
    "\n",
    "![](3-attach-files/regular_stacked_lstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "data_dim = 16\n",
    "timesteps = 8\n",
    "nb_classes = 10\n",
    "\n",
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True,\n",
    "              input_shape=(timesteps, data_dim))) # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(32, return_sequences=True)) # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(32)) # return a single vector of dimension 32\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# generate dummy training data\n",
    "X_train = np.random.random((1000, timesteps, data_dim))\n",
    "y_train = np.random.random((1000, nb_classes))\n",
    "\n",
    "# generate dummy validation data\n",
    "X_val = np.random.random((100, timesteps, data_dim))\n",
    "y_val = np.random.random((100, nb_classes))\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "         batch_size=64, nb_epoch=5,\n",
    "         validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 采用状态LSTM的相同模型\n",
    "\n",
    "状态(stateful)LSTM的特点是，在处理过一个batch的训练数据后，其内部状态(记忆)会被作为下一个batch的训练数据的初始状态。状态LSTM使得我们可以在合理的计算复杂度内处理较长序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "data_dim = 16\n",
    "timesteps = 8\n",
    "nb_classes = 10\n",
    "batch_size = 32\n",
    "\n",
    "# expected input batch shape: (batch_size, timesteps, data_dim)\n",
    "# note that we have to provide the full batch_input_shape since the network is stateful.\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True, stateful=True,\n",
    "              batch_input_shape=(batch_size, timesteps, data_dim)))\n",
    "model.add(LSTM(32, return_sequences=True, stateful=True))\n",
    "model.add(LSTM(32, stateful=True))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# generate dummy training data\n",
    "x_train = np.random.random((batch_size * 10, timesteps, data_dim))\n",
    "y_train = np.random.random((batch_size * 10, nb_classes))\n",
    "\n",
    "# generate dummy validation data\n",
    "x_val = np.random.random((batch_size * 3, timesteps, data_dim))\n",
    "y_val = np.random.random((batch_size * 3, nb_classes))\n",
    "\n",
    "model.fit(x_train, y_train, \n",
    "         batch_size=batch_size, nb_epoch=5,\n",
    "         validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将两个LSTM合并作为编码端来处理两路序列的分类\n",
    "\n",
    "在本模型中，两路输入序列通过两个LSTM被编码为特征向量。\n",
    "\n",
    "两路特征向量被串联在一起，然后通过一个全连接网络得到结果，示意图如下：\n",
    "\n",
    "![](3-attach-files/dual_lstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Merge, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "data_dim = 16\n",
    "timesteps = 8\n",
    "nb_classes = 10\n",
    "\n",
    "encoder_a = Sequential()\n",
    "encoder_a.add(LSTM(32, input_shape=(timesteps, data_dim)))\n",
    "\n",
    "encoder_b = Sequential()\n",
    "encoder_b.add(LSTM(32, input_shape=(timesteps, data_dim)))\n",
    "\n",
    "decoder = Sequential()\n",
    "decoder.add(Merge([encoder_a, encoder_b], mode='concat'))\n",
    "decoder.add(Dense(32, activation='relu'))\n",
    "decoder.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "decoder.compile(loss='categorical_crossentropy',\n",
    "               optimizer='rmsprop',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "# generate dummy training data\n",
    "x_train_a = np.random.random((1000, timesteps, data_dim))\n",
    "x_train_b = np.random.random((1000, timesteps, data_dim))\n",
    "y_train = np.random.random((1000, nb_classes))\n",
    "\n",
    "# generate dummy validation data\n",
    "x_val_a = np.random.random((100, timesteps, data_dim))\n",
    "x_val_b = np.random.random((100, timesteps, data_dim))\n",
    "y_val = np.random.random((100, nb_classes))\n",
    "\n",
    "decoder.fit([x_train_a, x_train_b], y_train,\n",
    "           batch_size=64, nb_epoch=5,\n",
    "           validation_data=([x_val_a, x_val_b], y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
